{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, f1_score, recall_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Time         V1         V2        V3        V4        V5  \\\n",
      "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
      "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
      "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
      "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
      "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
      "5            2.0  -0.425966   0.960523  1.141109 -0.168252  0.420987   \n",
      "6            4.0   1.229658   0.141004  0.045371  1.202613  0.191881   \n",
      "7            7.0  -0.644269   1.417964  1.074380 -0.492199  0.948934   \n",
      "8            7.0  -0.894286   0.286157 -0.113192 -0.271526  2.669599   \n",
      "9            9.0  -0.338262   1.119593  1.044367 -0.222187  0.499361   \n",
      "10          10.0   1.449044  -1.176339  0.913860 -1.375667 -1.971383   \n",
      "11          10.0   0.384978   0.616109 -0.874300 -0.094019  2.924584   \n",
      "12          10.0   1.249999  -1.221637  0.383930 -1.234899 -1.485419   \n",
      "13          11.0   1.069374   0.287722  0.828613  2.712520 -0.178398   \n",
      "14          12.0  -2.791855  -0.327771  1.641750  1.767473 -0.136588   \n",
      "15          12.0  -0.752417   0.345485  2.057323 -1.468643 -1.158394   \n",
      "16          12.0   1.103215  -0.040296  1.267332  1.289091 -0.735997   \n",
      "17          13.0  -0.436905   0.918966  0.924591 -0.727219  0.915679   \n",
      "18          14.0  -5.401258  -5.450148  1.186305  1.736239  3.049106   \n",
      "19          15.0   1.492936  -1.029346  0.454795 -1.438026 -1.555434   \n",
      "20          16.0   0.694885  -1.361819  1.029221  0.834159 -1.191209   \n",
      "21          17.0   0.962496   0.328461 -0.171479  2.109204  1.129566   \n",
      "22          18.0   1.166616   0.502120 -0.067300  2.261569  0.428804   \n",
      "23          18.0   0.247491   0.277666  1.185471 -0.092603 -1.314394   \n",
      "24          22.0  -1.946525  -0.044901 -0.405570 -1.013057  2.941968   \n",
      "25          22.0  -2.074295  -0.121482  1.322021  0.410008  0.295198   \n",
      "26          23.0   1.173285   0.353498  0.283905  1.133563 -0.172577   \n",
      "27          23.0   1.322707  -0.174041  0.434555  0.576038 -0.836758   \n",
      "28          23.0  -0.414289   0.905437  1.727453  1.473471  0.007443   \n",
      "29          23.0   1.059387  -0.175319  1.266130  1.186110 -0.786002   \n",
      "...          ...        ...        ...       ...       ...       ...   \n",
      "284777  172764.0   2.079137  -0.028723 -1.343392  0.358000 -0.045791   \n",
      "284778  172764.0  -0.764523   0.588379 -0.907599 -0.418847  0.901528   \n",
      "284779  172766.0   1.975178  -0.616244 -2.628295 -0.406246  2.327804   \n",
      "284780  172766.0  -1.727503   1.108356  2.219561  1.148583 -0.884199   \n",
      "284781  172766.0  -1.139015  -0.155510  1.894478 -1.138957  1.451777   \n",
      "284782  172767.0  -0.268061   2.540315 -1.400915  4.846661  0.639105   \n",
      "284783  172768.0  -1.796092   1.929178 -2.828417 -1.689844  2.199572   \n",
      "284784  172768.0  -0.669662   0.923769 -1.543167 -1.560729  2.833960   \n",
      "284785  172768.0   0.032887   0.545338 -1.185844 -1.729828  2.932315   \n",
      "284786  172768.0  -2.076175   2.142238 -2.522704 -1.888063  1.982785   \n",
      "284787  172769.0  -1.029719  -1.110670 -0.636179 -0.840816  2.424360   \n",
      "284788  172770.0   2.007418  -0.280235 -0.208113  0.335261 -0.715798   \n",
      "284789  172770.0  -0.446951   1.302212 -0.168583  0.981577  0.578957   \n",
      "284790  172771.0  -0.515513   0.971950 -1.014580 -0.677037  0.912430   \n",
      "284791  172774.0  -0.863506   0.874701  0.420358 -0.530365  0.356561   \n",
      "284792  172774.0  -0.724123   1.485216 -1.132218 -0.607190  0.709499   \n",
      "284793  172775.0   1.971002  -0.699067 -1.697541 -0.617643  1.718797   \n",
      "284794  172777.0  -1.266580  -0.400461  0.956221 -0.723919  1.531993   \n",
      "284795  172778.0 -12.516732  10.187818 -8.476671 -2.510473 -4.586669   \n",
      "284796  172780.0   1.884849  -0.143540 -0.999943  1.506772 -0.035300   \n",
      "284797  172782.0  -0.241923   0.712247  0.399806 -0.463406  0.244531   \n",
      "284798  172782.0   0.219529   0.881246 -0.635891  0.960928 -0.152971   \n",
      "284799  172783.0  -1.775135  -0.004235  1.189786  0.331096  1.196063   \n",
      "284800  172784.0   2.039560  -0.175233 -1.196825  0.234580 -0.008713   \n",
      "284801  172785.0   0.120316   0.931005 -0.546012 -0.745097  1.130314   \n",
      "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
      "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
      "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
      "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
      "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
      "\n",
      "              V6        V7        V8        V9  ...         V21       V22  \\\n",
      "0       0.462388  0.239599  0.098698  0.363787  ...   -0.018307  0.277838   \n",
      "1      -0.082361 -0.078803  0.085102 -0.255425  ...   -0.225775 -0.638672   \n",
      "2       1.800499  0.791461  0.247676 -1.514654  ...    0.247998  0.771679   \n",
      "3       1.247203  0.237609  0.377436 -1.387024  ...   -0.108300  0.005274   \n",
      "4       0.095921  0.592941 -0.270533  0.817739  ...   -0.009431  0.798278   \n",
      "5      -0.029728  0.476201  0.260314 -0.568671  ...   -0.208254 -0.559825   \n",
      "6       0.272708 -0.005159  0.081213  0.464960  ...   -0.167716 -0.270710   \n",
      "7       0.428118  1.120631 -3.807864  0.615375  ...    1.943465 -1.015455   \n",
      "8       3.721818  0.370145  0.851084 -0.392048  ...   -0.073425 -0.268092   \n",
      "9      -0.246761  0.651583  0.069539 -0.736727  ...   -0.246914 -0.633753   \n",
      "10     -0.629152 -1.423236  0.048456 -1.720408  ...   -0.009302  0.313894   \n",
      "11      3.317027  0.470455  0.538247 -0.558895  ...    0.049924  0.238422   \n",
      "12     -0.753230 -0.689405 -0.227487 -2.094011  ...   -0.231809 -0.483285   \n",
      "13      0.337544 -0.096717  0.115982 -0.221083  ...   -0.036876  0.074412   \n",
      "14      0.807596 -0.422911 -1.907107  0.755713  ...    1.151663  0.222182   \n",
      "15     -0.077850 -0.608581  0.003603 -0.436167  ...    0.499625  1.353650   \n",
      "16      0.288069 -0.586057  0.189380  0.782333  ...   -0.024612  0.196002   \n",
      "17     -0.127867  0.707642  0.087962 -0.665271  ...   -0.194796 -0.672638   \n",
      "18     -1.763406 -1.559738  0.160842  1.233090  ...   -0.503600  0.984460   \n",
      "19     -0.720961 -1.080664 -0.053127 -1.978682  ...   -0.177650 -0.175074   \n",
      "20      1.309109 -0.878586  0.445290 -0.446196  ...   -0.295583 -0.571955   \n",
      "21      1.696038  0.107712  0.521502 -1.191311  ...    0.143997  0.402492   \n",
      "22      0.089474  0.241147  0.138082 -0.989162  ...    0.018702 -0.061972   \n",
      "23     -0.150116 -0.946365 -1.617935  1.544071  ...    1.650180  0.200454   \n",
      "24      2.955053 -0.063063  0.855546  0.049967  ...   -0.579526 -0.799229   \n",
      "25     -0.959537  0.543985 -0.104627  0.475664  ...   -0.403639 -0.227404   \n",
      "26     -0.916054  0.369025 -0.327260 -0.246651  ...    0.067003  0.227812   \n",
      "27     -0.831083 -0.264905 -0.220982 -1.071425  ...   -0.284376 -0.323357   \n",
      "28     -0.200331  0.740228 -0.029247 -0.593392  ...    0.077237  0.457331   \n",
      "29      0.578435 -0.767084  0.401046  0.699500  ...    0.013676  0.213734   \n",
      "...          ...       ...       ...       ...  ...         ...       ...   \n",
      "284777 -1.345452  0.227476 -0.378355  0.665911  ...    0.235758  0.829758   \n",
      "284778 -0.760802  0.758545  0.414698 -0.730854  ...    0.003530 -0.431876   \n",
      "284779  3.664740 -0.533297  0.842937  1.128798  ...    0.086043  0.543613   \n",
      "284780  0.793083 -0.527298  0.866429  0.853819  ...   -0.094708  0.236818   \n",
      "284781  0.093598  0.191353  0.092211 -0.062621  ...   -0.191027 -0.631658   \n",
      "284782  0.186479 -0.045911  0.936448 -2.419986  ...   -0.263889 -0.857904   \n",
      "284783  3.123732 -0.270714  1.657495  0.465804  ...    0.271170  1.145750   \n",
      "284784  3.240843  0.181576  1.282746 -0.893890  ...    0.183856  0.202670   \n",
      "284785  3.401529  0.337434  0.925377 -0.165663  ...   -0.266113 -0.716336   \n",
      "284786  3.732950 -1.217430 -0.536644  0.272867  ...    2.016666 -1.588269   \n",
      "284787 -2.956733  0.283610 -0.332656 -0.247488  ...    0.353722  0.488487   \n",
      "284788 -0.751373 -0.458972 -0.140140  0.959971  ...   -0.208260 -0.430347   \n",
      "284789 -0.605641  1.253430 -1.042610 -0.417116  ...    0.851800  0.305268   \n",
      "284790 -0.316187  0.396137  0.532364 -0.224606  ...   -0.280302 -0.849919   \n",
      "284791 -1.046238  0.757051  0.230473 -0.506856  ...   -0.108846 -0.480820   \n",
      "284792 -0.482638  0.548393  0.343003 -0.226323  ...    0.414621  1.307511   \n",
      "284793  3.911336 -1.259306  1.056209  1.315006  ...    0.188758  0.694418   \n",
      "284794 -1.788600  0.314741  0.004704  0.013857  ...   -0.157831 -0.883365   \n",
      "284795 -1.394465 -3.632516  5.498583  4.893089  ...   -0.944759 -1.565026   \n",
      "284796 -0.613638  0.190241 -0.249058  0.666458  ...    0.144008  0.634646   \n",
      "284797 -1.343668  0.929369 -0.206210  0.106234  ...   -0.228876 -0.514376   \n",
      "284798 -1.014307  0.427126  0.121340 -0.285670  ...    0.099936  0.337120   \n",
      "284799  5.519980 -1.518185  2.080825  1.159498  ...    0.103302  0.654850   \n",
      "284800 -0.726571  0.017050 -0.118228  0.435402  ...   -0.268048 -0.717211   \n",
      "284801 -0.235973  0.812722  0.115093 -0.204064  ...   -0.314205 -0.808520   \n",
      "284802 -2.606837 -4.918215  7.305334  1.914428  ...    0.213454  0.111864   \n",
      "284803  1.058415  0.024330  0.294869  0.584800  ...    0.214205  0.924384   \n",
      "284804  3.031260 -0.296827  0.708417  0.432454  ...    0.232045  0.578229   \n",
      "284805  0.623708 -0.686180  0.679145  0.392087  ...    0.265245  0.800049   \n",
      "284806 -0.649617  1.577006 -0.414650  0.486180  ...    0.261057  0.643078   \n",
      "\n",
      "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
      "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
      "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
      "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
      "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
      "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
      "5      -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080    3.67   \n",
      "6      -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168    4.99   \n",
      "7       0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   40.80   \n",
      "8      -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   93.20   \n",
      "9      -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076    3.68   \n",
      "10      0.027740  0.500512  0.251367 -0.129478  0.042850  0.016253    7.80   \n",
      "11      0.009130  0.996710 -0.767315 -0.492208  0.042472 -0.054337    9.99   \n",
      "12      0.084668  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50   \n",
      "13     -0.071407  0.104744  0.548265  0.104094  0.021491  0.021293   27.50   \n",
      "14      1.020586  0.028317 -0.232746 -0.235557 -0.164778 -0.030154   58.80   \n",
      "15     -0.256573 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99   \n",
      "16      0.013802  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99   \n",
      "17     -0.156858 -0.888386 -0.342413 -0.049027  0.079692  0.131024    0.89   \n",
      "18      2.458589  0.042119 -0.481631 -0.621272  0.392053  0.949594   46.80   \n",
      "19      0.040002  0.295814  0.332931 -0.220385  0.022298  0.007602    5.00   \n",
      "20     -0.050881 -0.304215  0.072001 -0.422234  0.086553  0.063499  231.71   \n",
      "21     -0.048508 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09   \n",
      "22     -0.103855 -0.370415  0.603200  0.108556 -0.040521 -0.011418    2.28   \n",
      "23     -0.185353  0.423073  0.820591 -0.227632  0.336634  0.250475   22.75   \n",
      "24      0.870300  0.983421  0.321201  0.149650  0.707519  0.014600    0.89   \n",
      "25      0.742435  0.398535  0.249212  0.274404  0.359969  0.243232   26.43   \n",
      "26     -0.150487  0.435045  0.724825 -0.337082  0.016368  0.030041   41.88   \n",
      "27     -0.037710  0.347151  0.559639 -0.280158  0.042335  0.028822   16.00   \n",
      "28     -0.038500  0.642522 -0.183891 -0.277464  0.182687  0.152665   33.00   \n",
      "29      0.014462  0.002951  0.294638 -0.395070  0.081461  0.024220   12.99   \n",
      "...          ...       ...       ...       ...       ...       ...     ...   \n",
      "284777 -0.002063  0.001344  0.262183 -0.105327 -0.022363 -0.060283    1.00   \n",
      "284778  0.141759  0.587119 -0.200998  0.267337 -0.152951 -0.065285   80.00   \n",
      "284779 -0.032129  0.768379  0.477688 -0.031833  0.014151 -0.066542   25.00   \n",
      "284780 -0.204280  1.158185  0.627801 -0.399981  0.510818  0.233265   30.00   \n",
      "284781 -0.147249  0.212931  0.354257 -0.241068 -0.161717 -0.149188   13.00   \n",
      "284782  0.235172 -0.681794 -0.668894  0.044657 -0.066751 -0.072447   12.82   \n",
      "284783  0.084783  0.721269 -0.529906 -0.240117  0.129126 -0.080620   11.46   \n",
      "284784 -0.373023  0.651122  1.073823  0.844590 -0.286676 -0.187719   40.00   \n",
      "284785  0.108519  0.688519 -0.460220  0.161939  0.265368  0.090245    1.79   \n",
      "284786  0.588482  0.632444 -0.201064  0.199251  0.438657  0.172923    8.95   \n",
      "284787  0.293632  0.107812 -0.935586  1.138216  0.025271  0.255347    9.99   \n",
      "284788  0.416765  0.064819 -0.608337  0.268436 -0.028069 -0.041367    3.99   \n",
      "284789 -0.148093 -0.038712  0.010209 -0.362666  0.503092  0.229921   60.50   \n",
      "284790  0.300245  0.000607 -0.376379  0.128660 -0.015205 -0.021486    9.81   \n",
      "284791 -0.074513 -0.003988 -0.113149  0.280378 -0.077310  0.023079   20.32   \n",
      "284792 -0.059545  0.242669 -0.665424 -0.269869 -0.170579 -0.030692    3.99   \n",
      "284793  0.163002  0.726365 -0.058282 -0.191813  0.061858 -0.043716    4.99   \n",
      "284794  0.088485 -0.076790 -0.095833  0.132720 -0.028468  0.126494    0.89   \n",
      "284795  0.890675 -1.253276  1.786717  0.320763  2.090712  1.232864    9.87   \n",
      "284796 -0.042114 -0.053206  0.316403 -0.461441  0.018265 -0.041068   60.00   \n",
      "284797  0.279598  0.371441 -0.559238  0.113144  0.131507  0.081265    5.49   \n",
      "284798  0.251791  0.057688 -1.508368  0.144023  0.181205  0.215243   24.05   \n",
      "284799 -0.348929  0.745323  0.704545 -0.127579  0.454379  0.130308   79.99   \n",
      "284800  0.297930 -0.359769 -0.315610  0.201114 -0.080826 -0.075071    2.68   \n",
      "284801  0.050343  0.102800 -0.435870  0.124079  0.217940  0.068803    2.69   \n",
      "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
      "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
      "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
      "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
      "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
      "\n",
      "        Class  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "5           0  \n",
      "6           0  \n",
      "7           0  \n",
      "8           0  \n",
      "9           0  \n",
      "10          0  \n",
      "11          0  \n",
      "12          0  \n",
      "13          0  \n",
      "14          0  \n",
      "15          0  \n",
      "16          0  \n",
      "17          0  \n",
      "18          0  \n",
      "19          0  \n",
      "20          0  \n",
      "21          0  \n",
      "22          0  \n",
      "23          0  \n",
      "24          0  \n",
      "25          0  \n",
      "26          0  \n",
      "27          0  \n",
      "28          0  \n",
      "29          0  \n",
      "...       ...  \n",
      "284777      0  \n",
      "284778      0  \n",
      "284779      0  \n",
      "284780      0  \n",
      "284781      0  \n",
      "284782      0  \n",
      "284783      0  \n",
      "284784      0  \n",
      "284785      0  \n",
      "284786      0  \n",
      "284787      0  \n",
      "284788      0  \n",
      "284789      0  \n",
      "284790      0  \n",
      "284791      0  \n",
      "284792      0  \n",
      "284793      0  \n",
      "284794      0  \n",
      "284795      0  \n",
      "284796      0  \n",
      "284797      0  \n",
      "284798      0  \n",
      "284799      0  \n",
      "284800      0  \n",
      "284801      0  \n",
      "284802      0  \n",
      "284803      0  \n",
      "284804      0  \n",
      "284805      0  \n",
      "284806      0  \n",
      "\n",
      "[284807 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution & Checking the Imbalance in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    284315\n",
      "1       492\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Class\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Cases: 492\n",
      "Valid Transactions: 284315\n"
     ]
    }
   ],
   "source": [
    "print('Fraud Cases: {}'.format(len(df[df['Class'] == 1])))\n",
    "print('Valid Transactions: {}'.format(len(df[df['Class'] == 0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons obtenu une dataset de taille : 284 807 observations et 31 variables. \n",
    "Sur les 284 807 transactions, nous avons eu 492 fraudes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(df['Class'],facecolor=(0, 0, 0, 0),linewidth=5,edgecolor=sns.color_palette(\"dark\", 3), label = \"Count\")\n",
    "plt.title(\"Class Count\", fontsize=18)\n",
    "plt.xlabel(\"Is fraud?\", fontsize=15)\n",
    "plt.ylabel(\"Count\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "      <th>Amount(Normalized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149.62</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.69</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>378.66</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123.50</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.99</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount  Amount(Normalized)\n",
       "0  149.62            0.244964\n",
       "1    2.69           -0.342475\n",
       "2  378.66            1.160686\n",
       "3  123.50            0.140534\n",
       "4   69.99           -0.073403"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Amount(Normalized)'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df.iloc[:,[29,31]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['Amount', 'Time'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le jeu de données (Credit card) contient les transactions effectuées par cartes de crédit par les titulaires de carte européennes. Cet ensemble de données présente des transactions, où nous avons eu 492 fraudes sur 284 807 transactions. \n",
    "### L'ensemble de données est très déséquilibré, les classes positives (fraudes) représentent 0,172% de toutes les transactions. Il contient uniquement des variables d'entrée numériques résultant d'une transformation PCA. Les caractéristiques V1, V2, ... V28 sont les composantes principales obtenues avec PCA, les seules caractéristiques qui n'ont pas été transformées avec PCA sont 'Time' et 'Amount'. \n",
    "### La variable 'Time' contient les secondes écoulées entre chaque transaction et la première transaction de l'ensemble de données.\n",
    "### La variable 'Amount' est le Montant de la transaction, cette caractéristique peut être utilisée pour l'apprentissage sensible aux coûts dépendant de l'exemple.\n",
    "### La fonction 'Class' est la variable de réponse et prend la valeur 1 en cas de fraude et 0 sinon. Compte tenu du rapport de déséquilibre de classes, nous recommandons de mesurer la précision en utilisant l'aire sous la courbe de rappel de précision (AUPRC). \n",
    "### La précision de la matrice de confusion n'est pas significative pour une classification non équilibrée.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df.drop(['Class'], axis=1)\n",
    "#print(X)\n",
    "y= df['Class']\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction train_test_split utilise un randomiseur pour séparer les données en ensembles d'apprentissage et de test. 70% des données concernent l'ensemble d'apprentrissage et 30% 'enseble de test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Découpage des données en train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction RunModel prend en entrée le modèle non entrainé ainsi que toutes les données de test et de train, y compris les labels.\n",
    "Elle entrain le modèle, exécute la prédiction à l'aide des données de test et renvoie la matrice de confusion avec les étiquettes prédites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(model, X_train, y_train, X_test, y_test):\n",
    "   model.fit(X_train, y_train.values.ravel())\n",
    "   pred = model.predict(X_test)\n",
    "   matrix = confusion_matrix(y_test, pred)\n",
    "   return matrix, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PrintStats prend comme paramètres une matrice de confusion, des labels de test et des labels de prédiction et effectue les opérations suivantes:\n",
    "- Sépare la matrice de confusion en ses parties constituantes.\n",
    "- Calcule les scores F1, Recall, Accuracy et Cohen Kappa.\n",
    "- Imprime la matrice de confusion et tous les scores calculés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrintStats(cmat, y_test, pred):\n",
    "   # separate out the confusion matrix components\n",
    "   tpos = cmat[0][0]\n",
    "   fneg = cmat[1][1]\n",
    "   fpos = cmat[0][1]\n",
    "   tneg = cmat[1][0]\n",
    "   # calculate F!, Recall scores\n",
    "   f1Score = round(f1_score(y_test, pred), 2)\n",
    "   recallScore = round(recall_score(y_test, pred), 2)\n",
    "   # calculate and display metrics\n",
    "   print(cmat)\n",
    "   print( 'Accuracy: '+ str(np.round(100*float(tpos+fneg)/float(tpos+fneg + fpos + tneg),2))+'%')\n",
    "   print( 'Cohen Kappa: '+ str(np.round(cohen_kappa_score(y_test, pred),3)))\n",
    "   print(\"Sensitivity/Recall for Model : {recall_score}\".format(recall_score = recallScore))\n",
    "   print(\"F1 Score for Model : {f1_score}\".format(f1_score = f1Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous choisissons une technique d'apprentissage automatique (un modèle).\n",
    "La régression logistique est peut-être la technique d'apprentissage automatique à deux classes la plus courante. Nous l'utiliserons pour ce premier test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rihab\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85295    12]\n",
      " [   51    85]]\n",
      "Accuracy: 99.93%\n",
      "Cohen Kappa: 0.729\n",
      "Sensitivity/Recall for Model : 0.62\n",
      "F1 Score for Model : 0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "cmat, pred = RunModel(lr, X_train, y_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons obtenu 99,93% de ses prédictions correctes. \n",
    "Mais si nous regardons la matrices de confusions, nous remarquons que:\n",
    "- 85295 transactions ont été classées comme valides qui étaient réellement valides\n",
    "- 12 transactions ont été classées comme fraude réellement valide \n",
    "- 51 transactions ont été classées comme valides qui étaient des fraudes\n",
    "- 85 transactions ont été classées comme fraude qui étaient de la fraude\n",
    "Ainsi, bien que la précision soit excellente, nous constatons que l'algorithme a mal classé plus de 3 transactions frauduleuses sur 10\n",
    "\n",
    "En fait, si notre algorithme classait tout simplement comme valide, il aurait une précision meilleure que 99,9% mais serait totalement inutile!\n",
    "La précision n'est donc pas la mesure fiable de l'efficacité d'un modèle.\n",
    "Au lieu de cela, nous examinons d'autres mesures comme le score de Cohen Kappa, Recall et F1.\n",
    "Dans chaque cas, nous voulons obtenir un score aussi proche de 1 que possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85287    20]\n",
      " [   27   109]]\n",
      "Accuracy: 99.94%\n",
      "Cohen Kappa: 0.822\n",
      "Sensitivity/Recall for Model : 0.8\n",
      "F1 Score for Model : 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=10)\n",
    "cmat, pred = RunModel(dt, X_train, y_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons obtenu 99.94% de prédiction correcte avec un score de 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85298     9]\n",
      " [   29   107]]\n",
      "Accuracy: 99.96%\n",
      "Cohen Kappa: 0.849\n",
      "Sensitivity/Recall for Model : 0.79\n",
      "F1 Score for Model : 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "cmat, pred = RunModel(KNN, X_train, y_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons obtenu 99.96% de prédiction correcte avec un score de 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85300     7]\n",
      " [   48    88]]\n",
      "Accuracy: 99.94%\n",
      "Cohen Kappa: 0.762\n",
      "Sensitivity/Recall for Model : 0.65\n",
      "F1 Score for Model : 0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(gamma='auto')\n",
    "cmat, pred = RunModel(svc, X_train, y_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons obtenu 99.94% de prédiction correcte avec un score de 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85298     9]\n",
      " [   26   110]]\n",
      "Accuracy: 99.96%\n",
      "Cohen Kappa: 0.863\n",
      "Sensitivity/Recall for Model : 0.81\n",
      "F1 Score for Model : 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 150, n_jobs =4)\n",
    "cmat, pred = RunModel(rf, X_train, y_train, X_test, y_test)\n",
    "PrintStats(cmat, y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C’est un peu mieux.\n",
    "Notons que la précision a légèrement augmenté, aussi les autres scores ont également montré des améliorations significatives. \n",
    "Ainsi, une façon d'améliorer notre détection est d'essayer différents modèles et de voir comment ils fonctionnent.\n",
    "Changer clairement les modèles a aidé. Mais il y a aussi d'autres options. L'une consiste à suréchantillonner l'échantillon de dossiers de fraude ou, à l'inverse, à sous-échantillonner l'échantillon de bons dossiers. Le suréchantillonnage signifie ajouter des dossiers de fraude à notre échantillon de formation, augmentant ainsi la proportion globale des dossiers de fraude. À l'inverse, le sous-échantillonnage supprime des enregistrements valides de l'échantillon, ce qui a le même effet. La modification de l'échantillonnage rend l'algorithme plus «sensible» aux transactions frauduleuses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour plier au problématique des classes déséquilibrées, nous allons utiliser des méthodes de re-échantillonnage. \n",
    "#### Nous avons utilisé plusieurs méthodes under-sampling et over-sampling, mais la méthode SMOTE nous a procuré les meilleures performances. \n",
    "#### L’idée de SMOTE est d’augmenter le rappel pour la classe minoritaire en générant des individus synthétiques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    284315\n",
       "0    284315\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_smote, y_smote = sm.fit_sample(X, y)\n",
    "X_smote = pd.DataFrame(X_smote)\n",
    "y_smote = pd.DataFrame(y_smote)\n",
    "y_smote.iloc[:,0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote,X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote,y_smote, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85127    22]\n",
      " [    0 85440]]\n",
      "Accuracy: 99.99%\n",
      "Cohen Kappa: 1.0\n",
      "Sensitivity/Recall for Model : 1.0\n",
      "F1 Score for Model : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 150, n_jobs =4)\n",
    "cmat, pred = RunModel(rf, X_train_smote, y_train_smote, X_test_smote, y_test_smote)\n",
    "PrintStats(cmat, y_test_smote, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➢ Nous remarquons que la meilleur méthode et le meilleur algorithme pour notre jeu de données est le classifeur RandomForestClassifier avec la méthode smote.\n",
    "### ➢ L’utilisation de SMOTE noua a permis d’éviter le sur-apprentissage présent dans les autres méthodes de re-échantillonnage.\n",
    "### ➢ L’étude exploratoire sur les données a été bénéfique parce qu’elle nous a détecté la présence de variables non pertinentes et la déséquilibration des classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
